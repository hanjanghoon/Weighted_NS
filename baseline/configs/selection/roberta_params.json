{
    "dataset_args": {
      "n_candidates": 4,
      "eval_all_snippets": false,
      "negative_sample_method": "weighted",
  
      "history_max_utterances": 1000000,
      "history_max_tokens": 128,
      "knowledge_max_tokens": 128,
      "selection_type" : "all"
    },
    
  
    "task": "selection",
    "model_name_or_path": "roberta-base",
  
    "per_gpu_train_batch_size": 32,
    "per_gpu_eval_batch_size": 64,
    "gradient_accumulation_steps": 1,
    "max_candidates_per_forward_eval": 256,
    
    "learning_rate": 1e-5,
    "adam_epsilon": 1e-8,
    "max_grad_norm": 5.0,
  
    "num_train_epochs": 10,
    "warmup_steps": 0,
  
    "fp16": "",
  
    "seed": 42
  }
  